---
title: "Milestone Report"
author: "Florian Dollak"
date: "17 Mai 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo=F, message=F}
## libraries

library(tidyverse)
library(caret)
library(tm)
library(stringi)
library(SnowballC)
library(wordcloud)
library(openNLP)
library(plotly)
library(RWeka)
library(knitr)

## Parameters

set.seed(56315)                           # set random seed
```

```{r echo=F, message=F, cache=T, results='hide'}
## download the data

if (file.exists("Coursera-SwiftKey.zip") == F)
{
  download.file(
    "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip",
    destfile = "Coursera-SwiftKey.zip"
  )
  unzip("Coursera-SwiftKey.zip")
} else
  print("File already exists!")

## read the data

en_blogs_con <- file("./final/en_US/en_US.blogs.txt", "r")
en_blogs <-
  readLines("./final/en_US/en_US.blogs.txt", skipNul = T, encoding = "UTF-8")
close(en_blogs_con)

en_news_con <- file("./final/en_US/en_US.news.txt", "r")
en_news <-
  readLines(en_news_con,
            skipNul = T,
            warn = F,
            encoding = "UTF-8")
close(en_news_con)

en_twitter_con <- file("./final/en_US/en_US.twitter.txt", "r")
en_twitter  <-
  readLines(en_twitter_con, skipNul = T, encoding = "UTF-8")
close(en_twitter_con)
```

## Management Summary

This report gives a quick status overview over the creation of a prediction model for phrases and sentences. For this purpose an app will be created to demonstrate the model and how the next word is predicted based on previous user input.

The main goal of this app is to implement the model and predict the next word in common web-based contexts. To achieve this, a prediction model for the english language is constructed based on various text sources on the net. This report features the exploratory analysis of the provided text data and offers insight into the thought process behind the initial model creation.

## Basic Data Summary

First, an overview about basic properties of the data used. The initial datasets consist of english news-articles, blog-posts and twitter messages. After loading the data, samples are created for the first model to keep the processing effort low.

### Text Example

```{r echo=F}

en_news[38000]

```

### Word Counts, Line Counts and other info

```{r echo=F}
## Exploratory Analysis

# Word Counts

en_blogs_wc <- sum(stri_count_boundaries(en_blogs))
en_twitter_wc <- sum(stri_count_words(en_twitter))
en_news_wc <- sum(stri_count_words(en_news))

# Line Counts

en_blogs_lc <- length(en_blogs)
en_twitter_lc <- length(en_twitter)
en_news_lc <- length(en_news)

# File Size

en_blogs_fs <- file.size("./final/en_US/en_US.blogs.txt") / 1024 ^ 2
en_twitter_fs <-
  file.size("./final/en_US/en_US.twitter.txt") / 1024 ^ 2
en_news_fs <- file.size("./final/en_US/en_US.news.txt") / 1024 ^ 2


files_en <- data.frame(
  fileName = c("Blogs", "News", "Twitter"),
  fileSize = c(
    round(en_blogs_fs, digits = 2),
    round(en_twitter_fs, digits = 2),
    round(en_news_fs, digits = 2)
  ),
  lineCount = c(en_blogs_lc, en_twitter_lc, en_news_lc),
  wordCount = c(en_blogs_wc, en_twitter_wc, en_news_wc)
)

colnames(files_en) <-
  c("Text Source", "Filesize [MB]", "LineCount", "WordCount")

kable(files_en)
```


### Exploratory Graphs

Here an overview over the most common words in the text sample:

```{r echo=FALSE, cache=T}
## Corpus Creation

# create samples of the data

en_blogs_s <- sample(en_blogs, 5000)
en_news_s <- sample(en_news, 5000)
en_twitter_s <- sample(en_blogs, 5000)

en_all_s <- c(en_blogs_s, en_news_s, en_twitter_s)

## create a corpus from the sample data

Corpus_en <- VCorpus(VectorSource(en_all_s))

# clean the data

removeSpecialChars <- function(x) gsub('[^a-zA-Z0-9 ]', "", x, ignore.case = T, perl = T)
converttoutf8 <- function(x) iconv(x, to = "UTF-8", sub = "byte")

Corpus_en <- tm_map(Corpus_en, content_transformer(removePunctuation))
Corpus_en <- tm_map(Corpus_en, content_transformer(removeNumbers))
Corpus_en <- tm_map(Corpus_en, content_transformer(removeSpecialChars))
Corpus_en <- tm_map(Corpus_en, content_transformer(tolower))
Corpus_en <- tm_map(Corpus_en, removeWords, c(stopwords("english")))
Corpus_en <- tm_map(Corpus_en, content_transformer(stripWhitespace))
Corpus_en <- tm_map(Corpus_en, stemDocument, language = "english")
Corpus_en <- tm_map(Corpus_en,content_transformer(converttoutf8))

# Define the Document Term Matrix

dtm <- DocumentTermMatrix(Corpus_en)
```

```{r echo=F, fig.align="center"}
# Remove sparse items and measure frequencies

dtms <- removeSparseTerms(dtm, sparse = 0.955)

##Create Word Frequency Tables

freq <- sort(colSums(as.matrix((dtms))), decreasing = TRUE)
wf <- data.frame(word = names(freq), freq = freq)

## plot word frequencies
```

#### Word Cloud

```{r echo=FALSE, fig.align="center"}
# Create WordCloud

set.seed(1424)
dark2 <- brewer.pal(8, "Dark2")
wordcloud(
  names(freq),
  scale = c(4, 0.1),
  freq,
  max.words = 50,
  min.freq = 0,
  rot.per = 0.35,
  colors = dark2,
  random.order=FALSE
)
```

#### Histogram

```{r echo=F, fig.align="center", fig.width=10}

# Histogram

p <-
  ggplot(subset(wf, freq > 1), aes(reorder(word,-freq), freq)) + geom_bar(stat =
                                                                            "identity") +
  theme(axis.text = element_text(size = 10),
        axis.title = element_text(size = 10, face = "bold")) +
  theme(axis.text.x = element_text(size=12, angle = 45, hjust = 1)) +
  labs(x = "Terms", y = "Frequency", title = "Term Frequencies") + geom_text(aes(label =
                                                                                   freq), vjust = -1, size=3) +
  ylim(0, round(max(wf$freq)+0.05*max(wf$freq)))
p
```

#### Radar Plot

```{r echo=FALSE, fig.align="center"}
# Radar Plot

ggplot(wf, aes(x = word, y = freq, fill = word)) + geom_bar(
  width = 0.75,
  stat = "identity",
  colour = "black",
  size = 1
) + coord_polar(theta = "x") + xlab("") + ylab("") + ggtitle("Term Frequency") + theme(legend.position = "none") + labs(x = NULL, y = NULL)
```

### Most common 2- and 3-word phrases

And here also an overview about common phrases in the text sample. Some of the phrases are commonalities in the english language e.g. feel like, make sure. In the three word phrases there are also many names like "new york time", "presid barack obama" et cetera.

```{r echo=F, fig.align="center", fig.width=10}
## n-gram overview

# create the tokenizer function

Tokenizer2 <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
Tokenizer3 <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))

# Define the Document Term Matrices

dtm2 <- DocumentTermMatrix(Corpus_en, control = list(tokenize = Tokenizer2))
dtm3 <- DocumentTermMatrix(Corpus_en, control = list(tokenize = Tokenizer3))

# Remove sparse items and measure frequencies

dtms2 <- removeSparseTerms(dtm2, sparse = 0.997)
dtms3 <- removeSparseTerms(dtm3, sparse = 0.9996)

freq2 <- sort(colSums(as.matrix((dtms2))), decreasing = TRUE)
wf2 <- data.frame(word = names(freq2), freq = freq2)

freq3 <- sort(colSums(as.matrix((dtms3))), decreasing = TRUE)
wf3 <- data.frame(word = names(freq3), freq = freq3)

# Histogram

p2 <-
  ggplot(subset(wf2, freq > 1), aes(reorder(word,-freq), freq)) + geom_bar(stat =
                                                                            "identity") +
  theme(axis.text = element_text(size = 10),
        axis.title = element_text(size = 10, face = "bold")) +
  theme(axis.text.x = element_text(size=12, angle = 45, hjust = 1)) +
  labs(x = "Terms", y = "Frequency", title = "Term Frequencies") + geom_text(aes(label =
                                                                                   freq), vjust = -1, size=3) +
  ylim(0, round(max(wf2$freq)+0.05*max(wf2$freq)))
p2

p3 <-
  ggplot(subset(wf3, freq > 1), aes(reorder(word,-freq), freq)) + geom_bar(stat =
                                                                            "identity") +
  theme(axis.text = element_text(size = 10),
        axis.title = element_text(size = 10, face = "bold")) +
  theme(axis.text.x = element_text(size=12, angle = 45, hjust = 1)) +
  labs(x = "Terms", y = "Frequency", title = "Term Frequencies") + geom_text(aes(label =
                                                                                   freq), vjust = -1, size=3) +
  ylim(0, round(max(wf3$freq)+0.05*max(wf3$freq)))
p3
```

## Outline

The goal for the initial prediciton model is to expand the prediction to multiple-word phrases/sentences and generalize this concept for unseen words that do not appear in the training data. In order to maximize the accuracy of the model the plan is to use an extended training set to maximize model fit and subsequently finetune the model for performance to find the sweet spot between performance and accuracy.


